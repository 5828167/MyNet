{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = dsets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encode1 = nn.Linear(784, 128)\n",
    "        self.encode2 = nn.Linear(128, 30)\n",
    "        self.decode1 = nn.Linear(30, 128)\n",
    "        self.decode2 = nn.Linear(128, 784)\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.encode1(x))\n",
    "        encoded_out = self.activation(self.encode2(out))\n",
    "        out = self.activation(self.decode1(encoded_out))\n",
    "        out = self.decode2(out)\n",
    "        return encoded_out, out\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Iter [100/600] Loss: 0.0697\n",
      "Epoch [1/20], Iter [200/600] Loss: 0.0656\n",
      "Epoch [1/20], Iter [300/600] Loss: 0.0630\n",
      "Epoch [1/20], Iter [400/600] Loss: 0.0607\n",
      "Epoch [1/20], Iter [500/600] Loss: 0.0598\n",
      "Epoch [1/20], Iter [600/600] Loss: 0.0576\n",
      "Epoch [2/20], Iter [100/600] Loss: 0.0543\n",
      "Epoch [2/20], Iter [200/600] Loss: 0.0515\n",
      "Epoch [2/20], Iter [300/600] Loss: 0.0495\n",
      "Epoch [2/20], Iter [400/600] Loss: 0.0455\n",
      "Epoch [2/20], Iter [500/600] Loss: 0.0460\n",
      "Epoch [2/20], Iter [600/600] Loss: 0.0384\n",
      "Epoch [3/20], Iter [100/600] Loss: 0.0419\n",
      "Epoch [3/20], Iter [200/600] Loss: 0.0409\n",
      "Epoch [3/20], Iter [300/600] Loss: 0.0399\n",
      "Epoch [3/20], Iter [400/600] Loss: 0.0373\n",
      "Epoch [3/20], Iter [500/600] Loss: 0.0384\n",
      "Epoch [3/20], Iter [600/600] Loss: 0.0366\n",
      "Epoch [4/20], Iter [100/600] Loss: 0.0355\n",
      "Epoch [4/20], Iter [200/600] Loss: 0.0335\n",
      "Epoch [4/20], Iter [300/600] Loss: 0.0339\n",
      "Epoch [4/20], Iter [400/600] Loss: 0.0341\n",
      "Epoch [4/20], Iter [500/600] Loss: 0.0308\n",
      "Epoch [4/20], Iter [600/600] Loss: 0.0322\n",
      "Epoch [5/20], Iter [100/600] Loss: 0.0335\n",
      "Epoch [5/20], Iter [200/600] Loss: 0.0321\n",
      "Epoch [5/20], Iter [300/600] Loss: 0.0297\n",
      "Epoch [5/20], Iter [400/600] Loss: 0.0262\n",
      "Epoch [5/20], Iter [500/600] Loss: 0.0276\n",
      "Epoch [5/20], Iter [600/600] Loss: 0.0289\n",
      "Epoch [6/20], Iter [100/600] Loss: 0.0256\n",
      "Epoch [6/20], Iter [200/600] Loss: 0.0272\n",
      "Epoch [6/20], Iter [300/600] Loss: 0.0267\n",
      "Epoch [6/20], Iter [400/600] Loss: 0.0273\n",
      "Epoch [6/20], Iter [500/600] Loss: 0.0273\n",
      "Epoch [6/20], Iter [600/600] Loss: 0.0252\n",
      "Epoch [7/20], Iter [100/600] Loss: 0.0235\n",
      "Epoch [7/20], Iter [200/600] Loss: 0.0247\n",
      "Epoch [7/20], Iter [300/600] Loss: 0.0229\n",
      "Epoch [7/20], Iter [400/600] Loss: 0.0211\n",
      "Epoch [7/20], Iter [500/600] Loss: 0.0251\n",
      "Epoch [7/20], Iter [600/600] Loss: 0.0230\n",
      "Epoch [8/20], Iter [100/600] Loss: 0.0217\n",
      "Epoch [8/20], Iter [200/600] Loss: 0.0222\n",
      "Epoch [8/20], Iter [300/600] Loss: 0.0207\n",
      "Epoch [8/20], Iter [400/600] Loss: 0.0213\n",
      "Epoch [8/20], Iter [500/600] Loss: 0.0199\n",
      "Epoch [8/20], Iter [600/600] Loss: 0.0223\n",
      "Epoch [9/20], Iter [100/600] Loss: 0.0194\n",
      "Epoch [9/20], Iter [200/600] Loss: 0.0219\n",
      "Epoch [9/20], Iter [300/600] Loss: 0.0207\n",
      "Epoch [9/20], Iter [400/600] Loss: 0.0217\n",
      "Epoch [9/20], Iter [500/600] Loss: 0.0190\n",
      "Epoch [9/20], Iter [600/600] Loss: 0.0194\n",
      "Epoch [10/20], Iter [100/600] Loss: 0.0193\n",
      "Epoch [10/20], Iter [200/600] Loss: 0.0180\n",
      "Epoch [10/20], Iter [300/600] Loss: 0.0170\n",
      "Epoch [10/20], Iter [400/600] Loss: 0.0170\n",
      "Epoch [10/20], Iter [500/600] Loss: 0.0172\n",
      "Epoch [10/20], Iter [600/600] Loss: 0.0188\n",
      "Epoch [11/20], Iter [100/600] Loss: 0.0168\n",
      "Epoch [11/20], Iter [200/600] Loss: 0.0172\n",
      "Epoch [11/20], Iter [300/600] Loss: 0.0184\n",
      "Epoch [11/20], Iter [400/600] Loss: 0.0169\n",
      "Epoch [11/20], Iter [500/600] Loss: 0.0158\n",
      "Epoch [11/20], Iter [600/600] Loss: 0.0183\n",
      "Epoch [12/20], Iter [100/600] Loss: 0.0170\n",
      "Epoch [12/20], Iter [200/600] Loss: 0.0162\n",
      "Epoch [12/20], Iter [300/600] Loss: 0.0156\n",
      "Epoch [12/20], Iter [400/600] Loss: 0.0159\n",
      "Epoch [12/20], Iter [500/600] Loss: 0.0166\n",
      "Epoch [12/20], Iter [600/600] Loss: 0.0153\n",
      "Epoch [13/20], Iter [100/600] Loss: 0.0161\n",
      "Epoch [13/20], Iter [200/600] Loss: 0.0158\n",
      "Epoch [13/20], Iter [300/600] Loss: 0.0150\n",
      "Epoch [13/20], Iter [400/600] Loss: 0.0155\n",
      "Epoch [13/20], Iter [500/600] Loss: 0.0145\n",
      "Epoch [13/20], Iter [600/600] Loss: 0.0153\n",
      "Epoch [14/20], Iter [100/600] Loss: 0.0158\n",
      "Epoch [14/20], Iter [200/600] Loss: 0.0162\n",
      "Epoch [14/20], Iter [300/600] Loss: 0.0157\n",
      "Epoch [14/20], Iter [400/600] Loss: 0.0150\n",
      "Epoch [14/20], Iter [500/600] Loss: 0.0146\n",
      "Epoch [14/20], Iter [600/600] Loss: 0.0149\n",
      "Epoch [15/20], Iter [100/600] Loss: 0.0144\n",
      "Epoch [15/20], Iter [200/600] Loss: 0.0140\n",
      "Epoch [15/20], Iter [300/600] Loss: 0.0139\n",
      "Epoch [15/20], Iter [400/600] Loss: 0.0144\n",
      "Epoch [15/20], Iter [500/600] Loss: 0.0154\n",
      "Epoch [15/20], Iter [600/600] Loss: 0.0146\n",
      "Epoch [16/20], Iter [100/600] Loss: 0.0142\n",
      "Epoch [16/20], Iter [200/600] Loss: 0.0145\n",
      "Epoch [16/20], Iter [300/600] Loss: 0.0142\n",
      "Epoch [16/20], Iter [400/600] Loss: 0.0145\n",
      "Epoch [16/20], Iter [500/600] Loss: 0.0149\n",
      "Epoch [16/20], Iter [600/600] Loss: 0.0139\n",
      "Epoch [17/20], Iter [100/600] Loss: 0.0147\n",
      "Epoch [17/20], Iter [200/600] Loss: 0.0139\n",
      "Epoch [17/20], Iter [300/600] Loss: 0.0140\n",
      "Epoch [17/20], Iter [400/600] Loss: 0.0137\n",
      "Epoch [17/20], Iter [500/600] Loss: 0.0141\n",
      "Epoch [17/20], Iter [600/600] Loss: 0.0145\n",
      "Epoch [18/20], Iter [100/600] Loss: 0.0130\n",
      "Epoch [18/20], Iter [200/600] Loss: 0.0134\n",
      "Epoch [18/20], Iter [300/600] Loss: 0.0146\n",
      "Epoch [18/20], Iter [400/600] Loss: 0.0147\n",
      "Epoch [18/20], Iter [500/600] Loss: 0.0141\n",
      "Epoch [18/20], Iter [600/600] Loss: 0.0151\n",
      "Epoch [19/20], Iter [100/600] Loss: 0.0133\n",
      "Epoch [19/20], Iter [200/600] Loss: 0.0132\n",
      "Epoch [19/20], Iter [300/600] Loss: 0.0131\n",
      "Epoch [19/20], Iter [400/600] Loss: 0.0138\n",
      "Epoch [19/20], Iter [500/600] Loss: 0.0136\n",
      "Epoch [19/20], Iter [600/600] Loss: 0.0141\n",
      "Epoch [20/20], Iter [100/600] Loss: 0.0135\n",
      "Epoch [20/20], Iter [200/600] Loss: 0.0140\n",
      "Epoch [20/20], Iter [300/600] Loss: 0.0122\n",
      "Epoch [20/20], Iter [400/600] Loss: 0.0127\n",
      "Epoch [20/20], Iter [500/600] Loss: 0.0135\n",
      "Epoch [20/20], Iter [600/600] Loss: 0.0143\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        input = Variable(images.view(images.shape[0], -1))\n",
    "        targets=input.clone()\n",
    "        targets.require_grad=False\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        _, outputs = net(input)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=1, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "net.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in adv_test_loader:\n",
    "    input=Variable(images.view(images.shape[0], -1))\n",
    "    _, outputs = net(input)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5e302bbcd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e30334290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original')\n",
    "plt.imshow(images.numpy().reshape(28, 28), cmap='gray')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Reconstruct')\n",
    "plt.imshow(outputs.data.numpy().reshape(28, 28), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(30, 10)\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Iter [100/600] Loss: 1.5864\n",
      "Epoch [1/20], Iter [200/600] Loss: 1.1975\n",
      "Epoch [1/20], Iter [300/600] Loss: 0.9104\n",
      "Epoch [1/20], Iter [400/600] Loss: 0.7463\n",
      "Epoch [1/20], Iter [500/600] Loss: 0.8735\n",
      "Epoch [1/20], Iter [600/600] Loss: 0.6497\n",
      "Epoch [2/20], Iter [100/600] Loss: 0.5015\n",
      "Epoch [2/20], Iter [200/600] Loss: 0.5365\n",
      "Epoch [2/20], Iter [300/600] Loss: 0.7074\n",
      "Epoch [2/20], Iter [400/600] Loss: 0.4936\n",
      "Epoch [2/20], Iter [500/600] Loss: 0.4348\n",
      "Epoch [2/20], Iter [600/600] Loss: 0.5646\n",
      "Epoch [3/20], Iter [100/600] Loss: 0.6004\n",
      "Epoch [3/20], Iter [200/600] Loss: 0.5796\n",
      "Epoch [3/20], Iter [300/600] Loss: 0.4694\n",
      "Epoch [3/20], Iter [400/600] Loss: 0.4307\n",
      "Epoch [3/20], Iter [500/600] Loss: 0.5508\n",
      "Epoch [3/20], Iter [600/600] Loss: 0.4781\n",
      "Epoch [4/20], Iter [100/600] Loss: 0.4999\n",
      "Epoch [4/20], Iter [200/600] Loss: 0.4817\n",
      "Epoch [4/20], Iter [300/600] Loss: 0.4068\n",
      "Epoch [4/20], Iter [400/600] Loss: 0.5050\n",
      "Epoch [4/20], Iter [500/600] Loss: 0.5544\n",
      "Epoch [4/20], Iter [600/600] Loss: 0.4273\n",
      "Epoch [5/20], Iter [100/600] Loss: 0.3403\n",
      "Epoch [5/20], Iter [200/600] Loss: 0.3547\n",
      "Epoch [5/20], Iter [300/600] Loss: 0.3749\n",
      "Epoch [5/20], Iter [400/600] Loss: 0.3517\n",
      "Epoch [5/20], Iter [500/600] Loss: 0.4124\n",
      "Epoch [5/20], Iter [600/600] Loss: 0.4042\n",
      "Epoch [6/20], Iter [100/600] Loss: 0.3405\n",
      "Epoch [6/20], Iter [200/600] Loss: 0.3462\n",
      "Epoch [6/20], Iter [300/600] Loss: 0.3421\n",
      "Epoch [6/20], Iter [400/600] Loss: 0.3624\n",
      "Epoch [6/20], Iter [500/600] Loss: 0.5291\n",
      "Epoch [6/20], Iter [600/600] Loss: 0.5596\n",
      "Epoch [7/20], Iter [100/600] Loss: 0.3731\n",
      "Epoch [7/20], Iter [200/600] Loss: 0.4963\n",
      "Epoch [7/20], Iter [300/600] Loss: 0.3768\n",
      "Epoch [7/20], Iter [400/600] Loss: 0.2954\n",
      "Epoch [7/20], Iter [500/600] Loss: 0.3092\n",
      "Epoch [7/20], Iter [600/600] Loss: 0.4524\n",
      "Epoch [8/20], Iter [100/600] Loss: 0.3267\n",
      "Epoch [8/20], Iter [200/600] Loss: 0.4920\n",
      "Epoch [8/20], Iter [300/600] Loss: 0.4320\n",
      "Epoch [8/20], Iter [400/600] Loss: 0.3753\n",
      "Epoch [8/20], Iter [500/600] Loss: 0.5505\n",
      "Epoch [8/20], Iter [600/600] Loss: 0.4364\n",
      "Epoch [9/20], Iter [100/600] Loss: 0.2775\n",
      "Epoch [9/20], Iter [200/600] Loss: 0.2851\n",
      "Epoch [9/20], Iter [300/600] Loss: 0.4168\n",
      "Epoch [9/20], Iter [400/600] Loss: 0.3967\n",
      "Epoch [9/20], Iter [500/600] Loss: 0.6887\n",
      "Epoch [9/20], Iter [600/600] Loss: 0.3500\n",
      "Epoch [10/20], Iter [100/600] Loss: 0.3785\n",
      "Epoch [10/20], Iter [200/600] Loss: 0.5402\n",
      "Epoch [10/20], Iter [300/600] Loss: 0.2820\n",
      "Epoch [10/20], Iter [400/600] Loss: 0.3857\n",
      "Epoch [10/20], Iter [500/600] Loss: 0.3574\n",
      "Epoch [10/20], Iter [600/600] Loss: 0.3675\n",
      "Epoch [11/20], Iter [100/600] Loss: 0.3985\n",
      "Epoch [11/20], Iter [200/600] Loss: 0.4395\n",
      "Epoch [11/20], Iter [300/600] Loss: 0.2588\n",
      "Epoch [11/20], Iter [400/600] Loss: 0.5187\n",
      "Epoch [11/20], Iter [500/600] Loss: 0.2463\n",
      "Epoch [11/20], Iter [600/600] Loss: 0.5929\n",
      "Epoch [12/20], Iter [100/600] Loss: 0.3004\n",
      "Epoch [12/20], Iter [200/600] Loss: 0.3748\n",
      "Epoch [12/20], Iter [300/600] Loss: 0.3779\n",
      "Epoch [12/20], Iter [400/600] Loss: 0.3108\n",
      "Epoch [12/20], Iter [500/600] Loss: 0.3927\n",
      "Epoch [12/20], Iter [600/600] Loss: 0.3939\n",
      "Epoch [13/20], Iter [100/600] Loss: 0.4307\n",
      "Epoch [13/20], Iter [200/600] Loss: 0.4192\n",
      "Epoch [13/20], Iter [300/600] Loss: 0.6023\n",
      "Epoch [13/20], Iter [400/600] Loss: 0.4979\n",
      "Epoch [13/20], Iter [500/600] Loss: 0.3397\n",
      "Epoch [13/20], Iter [600/600] Loss: 0.2884\n",
      "Epoch [14/20], Iter [100/600] Loss: 0.3005\n",
      "Epoch [14/20], Iter [200/600] Loss: 0.5644\n",
      "Epoch [14/20], Iter [300/600] Loss: 0.5292\n",
      "Epoch [14/20], Iter [400/600] Loss: 0.4151\n",
      "Epoch [14/20], Iter [500/600] Loss: 0.5846\n",
      "Epoch [14/20], Iter [600/600] Loss: 0.4410\n",
      "Epoch [15/20], Iter [100/600] Loss: 0.4553\n",
      "Epoch [15/20], Iter [200/600] Loss: 0.3582\n",
      "Epoch [15/20], Iter [300/600] Loss: 0.4791\n",
      "Epoch [15/20], Iter [400/600] Loss: 0.4794\n",
      "Epoch [15/20], Iter [500/600] Loss: 0.4655\n",
      "Epoch [15/20], Iter [600/600] Loss: 0.5517\n",
      "Epoch [16/20], Iter [100/600] Loss: 0.2949\n",
      "Epoch [16/20], Iter [200/600] Loss: 0.3982\n",
      "Epoch [16/20], Iter [300/600] Loss: 0.3233\n",
      "Epoch [16/20], Iter [400/600] Loss: 0.4061\n",
      "Epoch [16/20], Iter [500/600] Loss: 0.5336\n",
      "Epoch [16/20], Iter [600/600] Loss: 0.3876\n",
      "Epoch [17/20], Iter [100/600] Loss: 0.2787\n",
      "Epoch [17/20], Iter [200/600] Loss: 0.2853\n",
      "Epoch [17/20], Iter [300/600] Loss: 0.3574\n",
      "Epoch [17/20], Iter [400/600] Loss: 0.5114\n",
      "Epoch [17/20], Iter [500/600] Loss: 0.3719\n",
      "Epoch [17/20], Iter [600/600] Loss: 0.3756\n",
      "Epoch [18/20], Iter [100/600] Loss: 0.3385\n",
      "Epoch [18/20], Iter [200/600] Loss: 0.5558\n",
      "Epoch [18/20], Iter [300/600] Loss: 0.4559\n",
      "Epoch [18/20], Iter [400/600] Loss: 0.2803\n",
      "Epoch [18/20], Iter [500/600] Loss: 0.4227\n",
      "Epoch [18/20], Iter [600/600] Loss: 0.4688\n",
      "Epoch [19/20], Iter [100/600] Loss: 0.3254\n",
      "Epoch [19/20], Iter [200/600] Loss: 0.6007\n",
      "Epoch [19/20], Iter [300/600] Loss: 0.3510\n",
      "Epoch [19/20], Iter [400/600] Loss: 0.2936\n",
      "Epoch [19/20], Iter [500/600] Loss: 0.2328\n",
      "Epoch [19/20], Iter [600/600] Loss: 0.3437\n",
      "Epoch [20/20], Iter [100/600] Loss: 0.2508\n",
      "Epoch [20/20], Iter [200/600] Loss: 0.3940\n",
      "Epoch [20/20], Iter [300/600] Loss: 0.2672\n",
      "Epoch [20/20], Iter [400/600] Loss: 0.3547\n",
      "Epoch [20/20], Iter [500/600] Loss: 0.2957\n",
      "Epoch [20/20], Iter [600/600] Loss: 0.4662\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        input = Variable(images.view(images.shape[0], -1))\n",
    "        targets=Variable(labels)\n",
    "        targets.require_grad=False\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        encoded, _ = net(input)\n",
    "        outputs = classifier(encoded)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5.48997879 -11.5638361   -7.14506483  -2.03127193  -4.75453568\n",
      "  -4.28232384 -11.4627552    8.03402519  -7.25605917  -0.34317207]\n",
      "Test Accuracy of the model on the 10000 test images: 94 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    input = Variable(images.view(images.shape[0], -1))\n",
    "    encoded, _ =net(input)\n",
    "    outputs = classifier(encoded)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    print(outputs.data.numpy()[0])\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "    break\n",
    "    \n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=1, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :9\n",
      "\n",
      "2 th example \n",
      "True value: 2\n",
      "Predicted value : 2\n",
      "Adversarial :6\n",
      "\n",
      "3 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :2\n",
      "\n",
      "4 th example \n",
      "True value: 0\n",
      "Predicted value : 0\n",
      "Adversarial :2\n",
      "\n",
      "5 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :7\n",
      "\n",
      "6 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :8\n",
      "\n",
      "7 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :5\n",
      "\n",
      "8 th example \n",
      "True value: 9\n",
      "Predicted value : 9\n",
      "Adversarial :5\n",
      "\n",
      "9 th example \n",
      "True value: 5\n",
      "Predicted value : 6\n",
      "Adversarial :6\n",
      "\n",
      "10 th example \n",
      "True value: 9\n",
      "Predicted value : 9\n",
      "Adversarial :7\n",
      "\n",
      "11 th example \n",
      "True value: 0\n",
      "Predicted value : 0\n",
      "Adversarial :5\n",
      "\n",
      "12 th example \n",
      "True value: 6\n",
      "Predicted value : 6\n",
      "Adversarial :2\n",
      "\n",
      "13 th example \n",
      "True value: 9\n",
      "Predicted value : 9\n",
      "Adversarial :4\n",
      "\n",
      "14 th example \n",
      "True value: 0\n",
      "Predicted value : 0\n",
      "Adversarial :5\n",
      "\n",
      "15 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :3\n",
      "\n",
      "16 th example \n",
      "True value: 5\n",
      "Predicted value : 5\n",
      "Adversarial :3\n",
      "\n",
      "17 th example \n",
      "True value: 9\n",
      "Predicted value : 9\n",
      "Adversarial :7\n",
      "\n",
      "18 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :3\n",
      "\n",
      "19 th example \n",
      "True value: 3\n",
      "Predicted value : 3\n",
      "Adversarial :6\n",
      "\n",
      "20 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :9\n",
      "\n",
      "21 th example \n",
      "True value: 9\n",
      "Predicted value : 9\n",
      "Adversarial :7\n",
      "\n",
      "22 th example \n",
      "True value: 6\n",
      "Predicted value : 6\n",
      "Adversarial :5\n",
      "\n",
      "23 th example \n",
      "True value: 6\n",
      "Predicted value : 6\n",
      "Adversarial :7\n",
      "\n",
      "24 th example \n",
      "True value: 5\n",
      "Predicted value : 5\n",
      "Adversarial :8\n",
      "\n",
      "25 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :7\n",
      "\n",
      "26 th example \n",
      "True value: 0\n",
      "Predicted value : 0\n",
      "Adversarial :8\n",
      "\n",
      "27 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :9\n",
      "\n",
      "28 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :9\n",
      "\n",
      "29 th example \n",
      "True value: 0\n",
      "Predicted value : 0\n",
      "Adversarial :3\n",
      "\n",
      "30 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :5\n",
      "\n",
      "31 th example \n",
      "True value: 3\n",
      "Predicted value : 3\n",
      "Adversarial :7\n",
      "\n",
      "32 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :5\n",
      "\n",
      "33 th example \n",
      "True value: 3\n",
      "Predicted value : 3\n",
      "Adversarial :5\n",
      "\n",
      "34 th example \n",
      "True value: 4\n",
      "Predicted value : 5\n",
      "Adversarial :0\n",
      "\n",
      "35 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :2\n",
      "\n",
      "36 th example \n",
      "True value: 2\n",
      "Predicted value : 2\n",
      "Adversarial :6\n",
      "\n",
      "37 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :9\n",
      "\n",
      "38 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :5\n",
      "\n",
      "39 th example \n",
      "True value: 2\n",
      "Predicted value : 2\n",
      "Adversarial :5\n",
      "\n",
      "40 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :8\n",
      "\n",
      "41 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :5\n",
      "\n",
      "42 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :9\n",
      "\n",
      "43 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :9\n",
      "\n",
      "44 th example \n",
      "True value: 2\n",
      "Predicted value : 2\n",
      "Adversarial :1\n",
      "\n",
      "45 th example \n",
      "True value: 3\n",
      "Predicted value : 3\n",
      "Adversarial :5\n",
      "\n",
      "46 th example \n",
      "True value: 5\n",
      "Predicted value : 5\n",
      "Adversarial :3\n",
      "\n",
      "47 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :3\n",
      "\n",
      "48 th example \n",
      "True value: 2\n",
      "Predicted value : 2\n",
      "Adversarial :6\n",
      "\n",
      "49 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :9\n",
      "\n",
      "50 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :7\n",
      "\n",
      "51 th example \n",
      "True value: 6\n",
      "Predicted value : 6\n",
      "Adversarial :3\n",
      "\n",
      "52 th example \n",
      "True value: 3\n",
      "Predicted value : 3\n",
      "Adversarial :5\n",
      "\n",
      "53 th example \n",
      "True value: 5\n",
      "Predicted value : 5\n",
      "Adversarial :3\n",
      "\n",
      "54 th example \n",
      "True value: 5\n",
      "Predicted value : 5\n",
      "Adversarial :3\n",
      "\n",
      "55 th example \n",
      "True value: 6\n",
      "Predicted value : 6\n",
      "Adversarial :2\n",
      "\n",
      "56 th example \n",
      "True value: 0\n",
      "Predicted value : 0\n",
      "Adversarial :8\n",
      "\n",
      "57 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :5\n",
      "\n",
      "58 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :9\n",
      "\n",
      "59 th example \n",
      "True value: 9\n",
      "Predicted value : 9\n",
      "Adversarial :4\n",
      "\n",
      "60 th example \n",
      "True value: 5\n",
      "Predicted value : 7\n",
      "Adversarial :7\n",
      "\n",
      "61 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :3\n",
      "\n",
      "62 th example \n",
      "True value: 8\n",
      "Predicted value : 8\n",
      "Adversarial :2\n",
      "\n",
      "63 th example \n",
      "True value: 9\n",
      "Predicted value : 4\n",
      "Adversarial :4\n",
      "\n",
      "64 th example \n",
      "True value: 3\n",
      "Predicted value : 2\n",
      "Adversarial :2\n",
      "\n",
      "65 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :2\n",
      "\n",
      "66 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :3\n",
      "\n",
      "67 th example \n",
      "True value: 6\n",
      "Predicted value : 6\n",
      "Adversarial :2\n",
      "\n",
      "68 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :7\n",
      "\n",
      "69 th example \n",
      "True value: 3\n",
      "Predicted value : 3\n",
      "Adversarial :8\n",
      "\n",
      "70 th example \n",
      "True value: 0\n",
      "Predicted value : 0\n",
      "Adversarial :5\n",
      "\n",
      "71 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :3\n",
      "\n",
      "72 th example \n",
      "True value: 0\n",
      "Predicted value : 0\n",
      "Adversarial :5\n",
      "\n",
      "73 th example \n",
      "True value: 2\n",
      "Predicted value : 2\n",
      "Adversarial :3\n",
      "\n",
      "74 th example \n",
      "True value: 9\n",
      "Predicted value : 9\n",
      "Adversarial :8\n",
      "\n",
      "75 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :5\n",
      "\n",
      "76 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :4\n",
      "\n",
      "77 th example \n",
      "True value: 3\n",
      "Predicted value : 3\n",
      "Adversarial :5\n",
      "\n",
      "78 th example \n",
      "True value: 2\n",
      "Predicted value : 2\n",
      "Adversarial :7\n",
      "\n",
      "79 th example \n",
      "True value: 9\n",
      "Predicted value : 9\n",
      "Adversarial :3\n",
      "\n",
      "80 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :8\n",
      "\n",
      "81 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :9\n",
      "\n",
      "82 th example \n",
      "True value: 6\n",
      "Predicted value : 6\n",
      "Adversarial :2\n",
      "\n",
      "83 th example \n",
      "True value: 2\n",
      "Predicted value : 2\n",
      "Adversarial :2\n",
      "\n",
      "84 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :9\n",
      "\n",
      "85 th example \n",
      "True value: 8\n",
      "Predicted value : 8\n",
      "Adversarial :5\n",
      "\n",
      "86 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :8\n",
      "\n",
      "87 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :8\n",
      "\n",
      "88 th example \n",
      "True value: 3\n",
      "Predicted value : 3\n",
      "Adversarial :5\n",
      "\n",
      "89 th example \n",
      "True value: 6\n",
      "Predicted value : 6\n",
      "Adversarial :4\n",
      "\n",
      "90 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :2\n",
      "\n",
      "91 th example \n",
      "True value: 3\n",
      "Predicted value : 3\n",
      "Adversarial :5\n",
      "\n",
      "92 th example \n",
      "True value: 6\n",
      "Predicted value : 6\n",
      "Adversarial :4\n",
      "\n",
      "93 th example \n",
      "True value: 9\n",
      "Predicted value : 4\n",
      "Adversarial :4\n",
      "\n",
      "94 th example \n",
      "True value: 3\n",
      "Predicted value : 3\n",
      "Adversarial :5\n",
      "\n",
      "95 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :8\n",
      "\n",
      "96 th example \n",
      "True value: 4\n",
      "Predicted value : 4\n",
      "Adversarial :8\n",
      "\n",
      "97 th example \n",
      "True value: 1\n",
      "Predicted value : 1\n",
      "Adversarial :5\n",
      "\n",
      "98 th example \n",
      "True value: 7\n",
      "Predicted value : 7\n",
      "Adversarial :8\n",
      "\n",
      "99 th example \n",
      "True value: 6\n",
      "Predicted value : 6\n",
      "Adversarial :5\n",
      "\n",
      "Accuracy of test_model : 6.000 , Adversarials : 93.000\n"
     ]
    }
   ],
   "source": [
    "dummy=1\n",
    "s=0.\n",
    "t=0.\n",
    "\n",
    "for (x, y) in adv_test_loader :\n",
    "    x, y = Variable(x, requires_grad=True), Variable(y, requires_grad=False)\n",
    "    encoded, _ = net(x.view(x.shape[0], -1))\n",
    "    probs = classifier(encoded)\n",
    "    y_pred = np.argmax(probs.data.numpy())\n",
    "    loss = nn.CrossEntropyLoss()(probs, y)\n",
    "    loss.backward()\n",
    "    epsilon = 0.1 \n",
    "    x_grad   = torch.sign(x.grad.data)\n",
    "    x_adversarial = torch.clamp(x.data + epsilon * x_grad, 0, 1)\n",
    "    adversarial_encoded, _ = net(Variable(x_adversarial.view(x.shape[0], -1)))\n",
    "    adversarial_probs = classifier(adversarial_encoded)\n",
    "    y_pred_adversarial = np.argmax(adversarial_probs.data.numpy())\n",
    "    \n",
    "    print(\"{0} th example \".format(dummy))\n",
    "    print (\"True value: \"+ str(y.data.numpy()[0])+\"\\nPredicted value : \"+str(y_pred)+ \"\\nAdversarial :\" + str(y_pred_adversarial)+\"\\n\" )\n",
    "    \n",
    "    dummy+=1\n",
    "    \n",
    "    if y.data.numpy()[0]!=y_pred :\n",
    "        t+=1\n",
    "    \n",
    "    if y_pred!=y_pred_adversarial :\n",
    "        s+=1\n",
    "    if dummy==100:\n",
    "        break\n",
    "        \n",
    "print(\"Accuracy of test_model : {0:.3f} , Adversarials : {1:.3f}\".format((t/dummy)*100, (s/dummy)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
